# -*- coding: utf-8 -*-
"""hw10_anomaly_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12D52GgTwb4k75mRCSM_y8ykqHvqk_gOJ

# **Homework 10 - Anomaly Detection**

若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com

# Load and preprocess data
"""

# !gdown --id '1_zT3JOpvXFGr7mkxs3XJDeGxTn_8pItq' --output train.npy
# !gdown --id '11Y_6JDjlhIY-M5-jW1rLRshDMqeKi9Kr' --output test.npy

from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,
                              TensorDataset)
from torch.optim import Adam, AdamW
from torch.utils.data import DataLoader
from torch.autograd import Variable
import torch.nn.functional as F
from torch import nn
import torch
from sklearn.decomposition import PCA
from scipy.cluster.vq import vq, kmeans
from sklearn.metrics import f1_score, pairwise_distances, roc_auc_score
from sklearn.cluster import MiniBatchKMeans
import numpy as np
import sys
import pandas as pd
import os
import matplotlib.pyplot as plt
torch.manual_seed(0)
np.random.seed(0)
# npy檔案就是紀錄矩陣的檔案
train = np.load(sys.argv[1], allow_pickle=True)
print("Size of training data:{}".format(len(train)))
# train = np.load(sys.argv[1], allow_pickle=True)
# test = np.load('test.npy', allow_pickle=True)
# print("Size of testing data:{}".format(len(test)))

if not os.path.isdir('./result/VAE'):
    os.makedirs('./result/VAE')
"""# Task

這份作業要執行的 task 是 semi-supervised anomaly detection，也就是說 training set 是乾淨的，testing 的時候才會混進 outlier data (anomaly)。
我們以某個簡單的 image dataset（image 加上他們的 label（分類））作為示範，training data 為原先 training set 中的某幾類，而 testing data 則是原先 testing set 的所有 data，要偵測的 anomaly 為 training data 中未出現的類別。label 的部分，1 為 outlier data，而 0 為 inlier data（相對於 outlier）。正確率以 AUC 計算。

方法以下列舉 3 種： KNN, PCA, Autoencoder。
"""

task = 'ae'




class fcn_autoencoder(nn.Module):
    # fully connected network
    def __init__(self):
        super(fcn_autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(32 * 32 * 3, 128),
            nn.ReLU(True),
            nn.Linear(128, 64),
            nn.ReLU(True),
            nn.Linear(64, 12),
            nn.ReLU(True),
            nn.Linear(12, 3))
        # 用encoder降維到3維
        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(True),
            nn.Linear(12, 64),
            nn.ReLU(True),
            nn.Linear(64, 128),
            nn.ReLU(True), nn.Linear(128, 32 * 32 * 3
            ), nn.Tanh())

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x


class conv_autoencoder(nn.Module):
    def __init__(self):
        super(conv_autoencoder, self).__init__()
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size)
        self.encoder = nn.Sequential(
            # [batch, 12, 16, 16]
            nn.Conv2d(3, 32, 3, stride=1, padding=1),
            nn.ReLU(),
            # [batch, 24, 8, 8]
            nn.Conv2d(32, 64, 3, stride=1, padding=1),
            nn.ReLU(),
			      # [batch, 48, 4, 4]
            nn.Conv2d(24, 48, 4, stride=2, padding=1),
            nn.ReLU(),
    # 			nn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]
    #       nn.ReLU(),
        )
        # ConvTranspose2d就是跟Conv2d座完全相反的事情 他的stride不會是filter真正的stride 而是Conv2d的stride
        self.decoder = nn.Sequential(
#             nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]
#             nn.ReLU(),
			      # [batch, 24, 8, 8]
			      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),
            nn.ReLU(),
			      # [batch, 12, 16, 16]
			      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),
            nn.ReLU(),
            # [batch, 3, 32, 32]
            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),
            nn.Tanh(),
        )
        self.fce1 = nn.Linear(48*4*4,48)
        self.relue_1 = nn.ReLU()
        self.fce2 = nn.Linear(48,10)
        
        self.fcd1 = nn.Linear(10,48)
        self.relu_d1 = nn.ReLU()
        self.fcd2 = nn.Linear(48,48*4*4)
        

    def forward(self, x):
        x = self.encoder(x)
        x = x.reshape(len(x),-1)
        x = self.fce1(x)
        x = self.relue_1(x)
        x = self.fce2(x)
        

        # Decoder
        x = self.fcd1(x)
        x = self.relu_d1(x)
        x = self.fcd2(x)
        x = x.reshape(len(x),48,4,4)
        x = self.decoder(x)
        return x


class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        self.downsampling = nn.Sequential(
            nn.Conv2d(3, 64, 4, stride=2, padding=1),            # [batch, 12, 16, 16]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),            # [batch, 12, 16, 16]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),            # [batch, 12, 16, 16]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),           # [batch, 24, 8, 8]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 256, 4, stride=2, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 512, 4, stride=2, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            # nn.Conv2d(512, 512, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            # nn.BatchNorm2d(512),
            # nn.ReLU(),
            # nn.Conv2d(512, 512, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            # nn.BatchNorm2d(512),
            # nn.ReLU(),
            
    # 			nn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]
    #       nn.ReLU(),
        )
    #     self.downsampling2 = nn.Sequential(
    #         nn.Conv2d(3, 32, 4, stride=2, padding=1),            # [batch, 12, 16, 16]
    #         nn.BatchNorm2d(32),
    #         nn.ReLU(),
    #         nn.Conv2d(32, 64, 4, stride=2, padding=1),           # [batch, 24, 8, 8]
    #         nn.BatchNorm2d(64),
    #         nn.ReLU(),
    #         nn.Conv2d(64, 128, 4, stride=2, padding=1),           # [batch, 48, 4, 4]
    #         nn.BatchNorm2d(128),
    #         nn.ReLU(),
    #         nn.Conv2d(128, 256, 4, stride=2, padding=1),           # [batch, 48, 4, 4]
    #         nn.BatchNorm2d(256),
    #         nn.ReLU(),

    # # 			nn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]
    # #       nn.ReLU(),
    #     )
        self.upsampling = nn.Sequential(
            # nn.ConvTranspose2d(256, 256, 3, stride=1, padding=1),            # [batch, 12, 16, 16]
            # nn.BatchNorm2d(256),
            # nn.ReLU(),
            # nn.ConvTranspose2d(256, 256, 3, stride=1, padding=1),            # [batch, 12, 16, 16]
            # nn.BatchNorm2d(256),
            # nn.ReLU(),

            # nn.ConvTranspose2d(512, 512, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            # nn.BatchNorm2d(512),
            # nn.ReLU(),
            # nn.ConvTranspose2d(512, 512, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            # nn.BatchNorm2d(512),
            # nn.ReLU(),
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(256),
            nn.ReLU(),

            nn.ConvTranspose2d(256, 256, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 256, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),            # [batch, 12, 16, 16]
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.ConvTranspose2d(128, 128, 3, stride=1, padding=1),           # [batch, 24, 8, 8]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),           # [batch, 24, 8, 8]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 64, 3, stride=1, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),           # [batch, 48, 4, 4]
            nn.BatchNorm2d(3),
            
            nn.Tanh()
    # 			nn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]
    #       nn.ReLU(),
        )
        # self.fc1 = nn.Linear(32*32*3, 400)
        self.fc21 = nn.Linear(512*2*2, 20)
        self.fc22 = nn.Linear(512*2*2, 20)
        self.fc3 = nn.Linear(20,512*2*2)
        # self.fc4 = nn.Linear(400, 48*4*4)
   
    def encode(self, x):
        x = x.reshape(len(x),3,32,32)
        h1 = self.downsampling(x)
        # h2 = self.downsampling2(x)
        
        h1 = h1.reshape(len(h1),-1)
        # h2 = h2.reshape(len(h2),-1)
        # h1 = F.relu(self.fc1(x))
        h2 = self.fc21(h1)
        h3 = self.fc22(h1)
        # return self.fc21(h1,dropout=0.5), self.fc22(h1,dropout=0.5)
        return h2, h3

    def reparametrize(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        if torch.cuda.is_available():
            
            eps = torch.cuda.FloatTensor(std.size()).normal_()
        else:
            eps = torch.FloatTensor(std.size()).normal_()
        eps = Variable(eps)
        # 將z從N(0,1) sample出來後 *上std 並加上u 就相當於是從N(mu,std^2) sample出來一樣
        return eps.mul(std).add_(mu)

    def decode(self, z):
        
        h3 = F.relu(self.fc3(z))
        # h3 = self.dropout3(h3)
        h3 = h3.reshape(len(h3),512,2,2)
        h3 = self.upsampling(h3)
        # return F.softmax(h3.reshape(len(h3),-1),dim=1)
        return h3.reshape(len(h3),-1)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparametrize(mu, logvar)
        return self.decode(z), mu, logvar
def loss_vae(recon_x, x, mu, logvar, criterion,epoch=1,num_epochs=1):
    """
    recon_x: generating images
    x: origin images
    mu: latent mean
    logvar: latent log variance
    """

    beta = 0.01+(0.99)*epoch/(num_epochs-1)
    
    
    # print(beta)
    mse = criterion(recon_x, x)  # mse loss
    # print(mse)
    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)
    KLD = torch.sum(KLD_element).mul_(-0.5)
    # KL divergence
    if epoch>=50:
        return mse + beta*KLD
    else:
        return mse


"""# Training"""

import time
if task == 'ae':
    # Auto Encoder
    num_epochs = 100
    batch_size = 128
    learning_rate = 1e-3

    # {'fcn', 'cnn', 'vae'}
    model_type = 'vae'
    train_loss = []
    x = train
    x = x.reshape(len(x), -1)

    data = torch.tensor(x, dtype=torch.float)
    train_dataset = TensorDataset(data)
    train_sampler = RandomSampler(train_dataset)
    train_dataloader = DataLoader(
        train_dataset, sampler=train_sampler, batch_size=batch_size)

    model_classes = {'fcn': fcn_autoencoder(
    ), 'cnn': conv_autoencoder(), 'vae': VAE()}
    model = model_classes[model_type].cuda()
    # model = torch.load(sys.argv[2])
    print("Auto-Encoder Model Type:{}".format(model_type))
    print("Total parameters:{}".format(sum(p.numel()
          for p in model.parameters())))
    print("Trainable parameters:{}".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))
    criterion=nn.MSELoss()
    optimizer=torch.optim.AdamW(
        model.parameters(), lr=learning_rate)
    # optimizer=torch.optim.SGD(
    #     model.parameters(), lr=learning_rate, momentum=0.9)

    best_loss=np.inf
    model.train()
    for epoch in range(num_epochs):
        LOSS = 0.0
        i=1
        start = time.time()
        for data in train_dataloader:
                # 這裡要做transpose應該是讓channel last-->channel first
            img=data[0].cuda()
            # ===================forward=====================
            
            output=model(img)
            # print(output)
            loss=loss_vae(output[0], img, output[1], output[2], criterion,epoch,num_epochs)
            # ===================backward====================
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # print(loss.item())
            LOSS += loss.item()
            i+=1
            # ===================save====================
            
        train_loss.append(LOSS/i)
        if LOSS/i < best_loss:
            best_loss = LOSS/i
            print("Save Model {} with loss {:^.4f}...".format(sys.argv[2], best_loss))
            torch.save(model, sys.argv[2])
        # ===================log========================
        print('epoch [{}/{}], {:.6f} Secs | loss:{:.4f}'
              .format(epoch + 1, num_epochs, time.time()-start, LOSS/i))
    train_loss=np.array(train_loss)
    df=pd.DataFrame(data=train_loss.reshape(
        1, -1), index=['Train Loss'], columns=['epoch'+str(i)for i in range(1, num_epochs+1)])
    df.to_csv('./result/VAE/train_loss.csv')
    train_loss_index=train_loss.argsort()
    print("The smallest loss is at epoch {} with MSE {:^.4f}".format(
        train_loss_index[0], train_loss[train_loss_index[0]]))
    plt.plot(np.arange(1, num_epochs+1), train_loss, 'b-')
    plt.title('Train Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig('./result/VAE/train_loss.png')
    plt.close()
"""# Evaluation

將 testing 的圖片輸入 model 後，可以得到其重建的圖片，並對兩者取平方差。可以發現 inlier 的平方差應該與 outlier 的平方差形成差距明顯的兩群數據。
"""
import os
import matplotlib.pyplot as plt
def normalize(reconstructed, test):
    # normalize the image to [0,1]
    max = np.max(reconstructed.reshape(len(reconstructed),-1),axis=1)
    min = np.min(reconstructed.reshape(len(reconstructed),-1),axis=1)
    reconstructed = (reconstructed - min[:,None,None,None])/(max[:,None,None,None]-min[:,None,None,None])

    max = np.max(test.reshape(len(test),-1),axis=1)
    min = np.min(test.reshape(len(test),-1),axis=1)
    test = (test - min[:,None,None,None])/(max[:,None,None,None]-min[:,None,None,None])
    # print(test<=1)
    return reconstructed, test
    
